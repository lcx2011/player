import os
import re
import subprocess
import requests
import json
import time
import locale
from functools import reduce
from hashlib import md5
from fastapi import FastAPI, HTTPException
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pathlib import Path
import asyncio
import aiohttp
from typing import Optional, Dict, List, Any
import random
import threading

# å¯¼å…¥é…ç½®
try:
    from config import BILIBILI_COOKIE
except ImportError:
    BILIBILI_COOKIE = ""
    print("è­¦å‘Š: æœªæ‰¾åˆ°config.pyæ–‡ä»¶ï¼Œå­—å¹•åŠŸèƒ½å°†ä¸å¯ç”¨")

# --- Configuration ---
BASE_DIR = Path(__file__).resolve().parent.parent
VIDEOS_DIR = BASE_DIR / "videos"
FRONTEND_DIR = BASE_DIR / "frontend"
COVERS_DIR = BASE_DIR / "covers"  # å°é¢ç¼“å­˜ç›®å½•
SUBTITLES_DIR = BASE_DIR / "subtitles"  # å­—å¹•ç¼“å­˜ç›®å½•
# Ensure the main directories exist
VIDEOS_DIR.mkdir(exist_ok=True)
COVERS_DIR.mkdir(exist_ok=True)
SUBTITLES_DIR.mkdir(exist_ok=True)

app = FastAPI(title="Video Player Backend")

# è®¾ç½®localeä»¥æ”¯æŒä¸­æ–‡æ’åº
try:
    locale.setlocale(locale.LC_COLLATE, 'zh_CN.UTF-8')
except locale.Error:
    try:
        locale.setlocale(locale.LC_COLLATE, 'Chinese (Simplified)_China.936')
    except locale.Error:
        try:
            locale.setlocale(locale.LC_COLLATE, 'zh_CN')
        except locale.Error:
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨é»˜è®¤æ’åº
            pass

# æŒ‚è½½å‰ç«¯é™æ€æ–‡ä»¶æœåŠ¡
app.mount("/frontend", StaticFiles(directory=str(FRONTEND_DIR)), name="frontend")

# --- CORS Middleware ---
# This allows the frontend (running on a different port) to communicate with this backend.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

# ä¸­æ–‡å‹å¥½çš„æ’åºå‡½æ•°
def chinese_sort_key(text: str) -> str:
    """ç”Ÿæˆä¸­æ–‡å‹å¥½çš„æ’åºé”®"""
    import unicodedata
    # å°†ä¸­æ–‡å­—ç¬¦è½¬æ¢ä¸ºæ‹¼éŸ³æˆ–è€…ä½¿ç”¨Unicodeåºå·
    normalized = unicodedata.normalize('NFKD', text)
    # ç®€å•çš„æ’åºç­–ç•¥ï¼šæ•°å­—ä¼˜å…ˆï¼Œç„¶åæ˜¯å­—æ¯ï¼Œæœ€åæ˜¯ä¸­æ–‡
    result = []
    for char in normalized:
        if char.isdigit():
            result.append(('0', char))  # æ•°å­—æ’åœ¨æœ€å‰
        elif char.isascii() and char.isalpha():
            result.append(('1', char.lower()))  # å­—æ¯æ’åœ¨ä¸­é—´
        else:
            result.append(('2', char))  # ä¸­æ–‡ç­‰å…¶ä»–å­—ç¬¦æ’åœ¨æœ€å
    return result

def sort_folders_chinese(folders: List[dict]) -> List[dict]:
    """æŒ‰ä¸­æ–‡å‹å¥½çš„æ–¹å¼æ’åºæ–‡ä»¶å¤¹"""
    try:
        # å°è¯•ä½¿ç”¨localeæ’åº
        return sorted(folders, key=lambda x: locale.strxfrm(x['name']))
    except (AttributeError, TypeError):
        # å¦‚æœlocaleæ’åºå¤±è´¥ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ’åº
        return sorted(folders, key=lambda x: chinese_sort_key(x['name']))

# --- Bilibili Downloader Logic (Adapted from 1.py) ---

HEADERS = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36',
    'referer': 'https://www.bilibili.com/'
}

# å…¨å±€å¼‚æ­¥HTTPå®¢æˆ·ç«¯
_http_session: Optional[aiohttp.ClientSession] = None

# å†…å­˜ç¼“å­˜
_video_parts_cache: Dict[str, Any] = {}
_wbi_key_cache: Optional[str] = None
_wbi_key_cache_time: float = 0

# --- Outbound request limiting & backoff ---
# æœ€å¤§å¹¶å‘å¤–å‘¼æ•°ï¼ˆæ ¹æ®å®é™…æƒ…å†µå¾®è°ƒï¼‰
_MAX_CONCURRENT_OUTBOUND = int(os.getenv("OUTBOUND_MAX_CONCURRENCY", "3"))
# ç›®æ ‡æ¯ç§’è¯·æ±‚æ•°ï¼ˆå…¨å±€ï¼‰ï¼Œé€šè¿‡è¯·æ±‚é—´éš”å®ç°ï¼ˆå¸¦æŠ–åŠ¨ï¼‰
_MAX_QPS = float(os.getenv("OUTBOUND_MAX_QPS", "2"))  # e.g. 2 req/s -> 500ms åŸºå‡†é—´éš”
# 429/403 å†·å´ç§’æ•°ä¸Šé™ï¼ˆæŒ‡æ•°é€€é¿ä¸­çš„æœ€å¤§å†·å´ï¼‰
_MAX_COOLDOWN_SECONDS = int(os.getenv("OUTBOUND_MAX_COOLDOWN", "300"))

# å¼‚æ­¥å¹¶å‘æ§åˆ¶ä¸æ—¶åºæ§åˆ¶
_outbound_sem = asyncio.Semaphore(_MAX_CONCURRENT_OUTBOUND)
_outbound_lock = asyncio.Lock()  # ä¿æŠ¤æ—¶é—´é—´éš”è°ƒåº¦
_next_earliest_ts = 0.0  # å•ä½ï¼šmonotonic ç§’

# åŒæ­¥ä»£ç è·¯å¾„ï¼ˆrequestsï¼‰ç”¨çš„é”ä¸æ—¶åºæ§åˆ¶
_sync_lock = threading.Lock()
_sync_next_earliest_ts = 0.0

# é’ˆå¯¹ç‰¹å®šç«¯ç‚¹çš„å†·å´çª—å£ï¼Œkey å¯ç”¨ä¸º URL å‰ç¼€
_cooldowns: Dict[str, float] = {}

def _endpoint_key(url: str) -> str:
    # ç®€åŒ–ï¼šå–ä¸»æœº+è·¯å¾„çš„å‰ä¸¤æ®µä½œä¸º keyï¼Œé¿å…è¿‡ç»†é¢—ç²’åº¦
    try:
        from urllib.parse import urlparse
        p = urlparse(url)
        parts = p.path.strip('/').split('/')
        head = '/'.join(parts[:2]) if parts else ''
        return f"{p.scheme}://{p.netloc}/{head}"
    except Exception:
        return url

def _in_cooldown(url: str) -> bool:
    now = time.monotonic()
    key = _endpoint_key(url)
    until = _cooldowns.get(key, 0.0)
    return now < until

def _set_cooldown(url: str, base_seconds: float) -> None:
    # å åŠ å†·å´åˆ°ä¸è¶…è¿‡æœ€å¤§ä¸Šé™
    now = time.monotonic()
    key = _endpoint_key(url)
    current = _cooldowns.get(key, 0.0)
    target = now + min(base_seconds, _MAX_COOLDOWN_SECONDS)
    if target > current:
        _cooldowns[key] = target

def _jitter(seconds: float) -> float:
    # æŠ–åŠ¨ï¼šÂ±20%
    delta = seconds * 0.2
    return max(0.0, seconds + random.uniform(-delta, delta))

async def _await_global_qps_window():
    global _next_earliest_ts
    async with _outbound_lock:
        now = time.monotonic()
        min_gap = 1.0 / max(_MAX_QPS, 0.0001)
        wait = max(0.0, _next_earliest_ts - now)
        if wait > 0:
            await asyncio.sleep(wait)
        # è®¾å®šä¸‹ä¸€æ¬¡æœ€æ—©æ—¶é—´ç‚¹ï¼ˆå¸¦æŠ–åŠ¨ï¼‰
        _next_earliest_ts = time.monotonic() + _jitter(min_gap)

async def limited_get(url: str, params: Optional[Dict]=None, headers: Optional[Dict]=None, retries: int=3) -> Optional[aiohttp.ClientResponse]:
    """å¸¦å¹¶å‘é™åˆ¶ã€QPS é—´éš”ã€é€€é¿ä¸å†·å´çš„ GETï¼ˆaiohttpï¼‰ã€‚è¿”å›å·²æ‰“å¼€çš„å“åº”å¯¹è±¡æˆ– Noneã€‚"""
    if _in_cooldown(url):
        return None
    session = await get_http_session()
    last_exc: Optional[Exception] = None
    backoff = 0.5  # åˆå§‹é€€é¿åŸºå‡†ï¼ˆç§’ï¼‰
    for attempt in range(retries):
        async with _outbound_sem:
            await _await_global_qps_window()
            try:
                resp = await session.get(url, params=params, headers=headers)
                if resp.status == 200:
                    return resp
                # å¯¹ 429/403ï¼šè¿›å…¥å†·å´å¹¶ç«‹å³ç»“æŸï¼ˆé¿å… hammerï¼‰
                if resp.status in (429, 403):
                    # ä»¥ 60s * 2^attempt é€’å¢ï¼Œå°é¡¶ _MAX_COOLDOWN_SECONDS
                    _set_cooldown(url, 60.0 * (2 ** attempt))
                    await resp.release()
                    return None
                # å…¶å®ƒ 5xx å¯é€€é¿é‡è¯•ï¼›4xxï¼ˆé429/403ï¼‰ç›´æ¥æ”¾å¼ƒ
                if 500 <= resp.status < 600:
                    last_exc = Exception(f"HTTP {resp.status}")
                else:
                    await resp.release()
                    return None
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                last_exc = e
        # é€€é¿ç­‰å¾…ï¼ˆå¸¦æŠ–åŠ¨ï¼‰
        await asyncio.sleep(_jitter(backoff))
        backoff = min(backoff * 2, 8.0)
    return None

def limited_get_sync(url: str, params: Optional[Dict]=None, headers: Optional[Dict]=None, timeout: int=15, retries: int=3):
    """åŒæ­¥è·¯å¾„çš„å—é™ GETï¼ˆrequestsï¼‰ï¼Œå¸¦å¹¶å‘é—´éš”ä¸é€€é¿ã€‚ç”±äº GILï¼Œæˆ‘ä»¬ä»…å®ç° QPS é—´éš”ä¸å†·å´ï¼Œä¸åšå¹¶å‘ä¿¡å·é‡ã€‚"""
    if _in_cooldown(url):
        return None
    last_exc: Optional[Exception] = None
    backoff = 0.5
    for attempt in range(retries):
        # å…¨å±€ QPS æ§åˆ¶ï¼ˆåŒæ­¥ï¼‰
        with _sync_lock:
            global _sync_next_earliest_ts
            now = time.monotonic()
            min_gap = 1.0 / max(_MAX_QPS, 0.0001)
            wait = max(0.0, _sync_next_earliest_ts - now)
            if wait > 0:
                time.sleep(wait)
            _sync_next_earliest_ts = time.monotonic() + _jitter(min_gap)
        try:
            resp = requests.get(url, params=params, headers=headers or HEADERS, timeout=timeout)
            status = resp.status_code
            if status == 200:
                return resp
            if status in (429, 403):
                _set_cooldown(url, 60.0 * (2 ** attempt))
                return None
            if 500 <= status < 600:
                last_exc = Exception(f"HTTP {status}")
            else:
                return None
        except requests.exceptions.RequestException as e:
            last_exc = e
        time.sleep(_jitter(backoff))
        backoff = min(backoff * 2, 8.0)
    return None

async def get_http_session() -> aiohttp.ClientSession:
    """è·å–å…¨å±€HTTPä¼šè¯"""
    global _http_session
    if _http_session is None or _http_session.closed:
        timeout = aiohttp.ClientTimeout(total=15)
        connector = aiohttp.TCPConnector(ssl=False)  # ç¦ç”¨SSLéªŒè¯
        _http_session = aiohttp.ClientSession(
            headers=HEADERS,
            timeout=timeout,
            connector=connector
        )
    return _http_session

async def close_http_session():
    """å…³é—­HTTPä¼šè¯"""
    global _http_session
    if _http_session and not _http_session.closed:
        await _http_session.close()
        _http_session = None



# WBIç­¾åç›¸å…³å¸¸é‡å’Œå‡½æ•°
MIXIN_KEY_ENC_TAB = [
    46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,
    33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,
    61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,
    36, 20, 34, 44, 52
]

def get_mixin_key(orig: str):
    return reduce(lambda s, i: s + orig[i], MIXIN_KEY_ENC_TAB, '')[:32]

async def get_wbi_keys_async(cookie: Optional[str] = None) -> Optional[str]:
    """å¼‚æ­¥è·å–WBIå¯†é’¥ï¼Œå¸¦ç¼“å­˜"""
    global _wbi_key_cache, _wbi_key_cache_time

    # æ£€æŸ¥ç¼“å­˜ï¼ˆ5åˆ†é’Ÿæœ‰æ•ˆæœŸï¼‰
    current_time = time.time()
    if _wbi_key_cache and (current_time - _wbi_key_cache_time) < 300:
        return _wbi_key_cache

    try:
        session = await get_http_session()
        headers = {}
        if cookie:
            headers['Cookie'] = cookie

        async with session.get('https://api.bilibili.com/x/web-interface/nav', headers=headers) as response:
            if response.status == 200:
                json_content = await response.json()
                img_url: str = json_content['data']['wbi_img']['img_url']
                sub_url: str = json_content['data']['wbi_img']['sub_url']
                img_key = img_url.rsplit('/', 1)[1].split('.')[0]
                sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]

                wbi_key = get_mixin_key(img_key + sub_key)

                # æ›´æ–°ç¼“å­˜
                _wbi_key_cache = wbi_key
                _wbi_key_cache_time = current_time

                return wbi_key
    except Exception as e:
        print(f"å¼‚æ­¥è·å–WBIå¯†é’¥å¤±è´¥: {e}")
    return None

def get_wbi_keys(cookie=None):
    try:
        headers = HEADERS.copy()
        if cookie:
            headers['Cookie'] = cookie
        resp = requests.get('https://api.bilibili.com/x/web-interface/nav', headers=headers)
        resp.raise_for_status()
        json_content = resp.json()
        img_url: str = json_content['data']['wbi_img']['img_url']
        sub_url: str = json_content['data']['wbi_img']['sub_url']
        img_key = img_url.rsplit('/', 1)[1].split('.')[0]
        sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]
        return get_mixin_key(img_key + sub_key)
    except Exception as e:
        print(f"è·å–WBIå¯†é’¥å¤±è´¥: {e}")
        return None

def sign_wbi_params(params: dict, wbi_key: str):
    params['wts'] = str(int(time.time()))
    sorted_params = dict(sorted(params.items()))
    query_parts = []
    for k, v in sorted_params.items():
        v_str = str(v).replace("'", "").replace("!", "").replace("(", "").replace(")", "").replace("*", "")
        query_parts.append(f'{k}={v_str}')
    query_string = '&'.join(query_parts)
    w_rid = md5((query_string + wbi_key).encode()).hexdigest()
    params['w_rid'] = w_rid
    return params

def convert_seconds_to_lrc_time(seconds):
    millisec = int((seconds - int(seconds)) * 100)
    minutes = int(seconds // 60)
    sec = int(seconds % 60)
    return f"[{minutes:02d}:{sec:02d}.{millisec:02d}]"

def sanitize_filename(name):
    return re.sub(r'[\\/*?:"<>|]', "", name)

async def get_bilibili_response_async(url: str, params: Optional[Dict] = None, retries: int = 3) -> Optional[aiohttp.ClientResponse]:
    """å¼‚æ­¥å‘é€è¯·æ±‚åˆ°Bç«™APIç«¯ç‚¹ï¼Œæ”¯æŒé‡è¯•ã€å¹¶å‘é™åˆ¶ä¸é€€é¿ã€‚"""
    resp = await limited_get(url, params=params, headers=None, retries=retries)
    if not resp:
        print(f"å¼‚æ­¥è¯·æ±‚å¤±è´¥æˆ–è¢«é™æµ: {url}")
    return resp

def get_bilibili_response(url, params=None, retries: int = 3):
    """å‘é€è¯·æ±‚åˆ°Bç«™APIç«¯ç‚¹ï¼ˆåŒæ­¥è·¯å¾„ï¼‰ï¼Œå¸¦é€€é¿/QPS é—´éš”/å†·å´ã€‚"""
    resp = limited_get_sync(url, params=params, headers=HEADERS, timeout=15, retries=retries)
    if not resp:
        print(f"è¯·æ±‚å¤±è´¥æˆ–è¢«é™æµ: {url}")
    return resp

def extract_bvid_from_url(url_or_bvid: str) -> str:
    """Extract BV ID from Bilibili URL or return as-is if already a BV ID."""
    if url_or_bvid.startswith('http'):
        # Extract BV ID from URL like https://www.bilibili.com/video/BV1LnuzzyEQp
        match = re.search(r'/video/(BV[a-zA-Z0-9]+)', url_or_bvid)
        if match:
            return match.group(1)
        else:
            raise ValueError(f"Could not extract BV ID from URL: {url_or_bvid}")
    else:
        # Assume it's already a BV ID
        return url_or_bvid

async def get_video_parts_with_covers_async(bvid: str) -> Optional[List[Dict]]:
    """å¼‚æ­¥è·å–è§†é¢‘åˆ†Pä¿¡æ¯å’Œå°é¢ï¼Œå¸¦ç¼“å­˜ä¸å¤–å‘¼é™æµ"""
    # æ£€æŸ¥ç¼“å­˜
    cache_key = f"parts_covers_{bvid}"
    if cache_key in _video_parts_cache:
        return _video_parts_cache[cache_key]

    try:
        url = f"https://www.bilibili.com/video/{bvid}"
        response = await limited_get(url)
        if not response:
            return None
        html_content = await response.text()

        # æå–JSONæ•°æ®
        match = re.search(r'<script>window\.__INITIAL_STATE__=(.*?);\(function\(\)', html_content)
        if not match:
            print(f"æœªæ‰¾åˆ°è§†é¢‘æ•°æ®: {bvid}")
            return None

        json_data_string = match.group(1)
        data = json.loads(json_data_string)

        # è·å–è§†é¢‘åˆ†Påˆ—è¡¨
        video_parts = data.get('videoData', {}).get('pages', [])
        if not video_parts:
            print(f"æœªæ‰¾åˆ°åˆ†Pè§†é¢‘: {bvid}")
            return None

        # ä¸ºæ¯ä¸ªåˆ†Pæ·»åŠ å°é¢ä¿¡æ¯
        enhanced_parts = []
        for part in video_parts:
            enhanced_part = {
                'cid': part.get('cid'),
                'page': part.get('page'),
                'part': part.get('part'),
                'duration': part.get('duration'),
                'cover_url': part.get('first_frame', ''),
                'dimension': part.get('dimension', {})
            }
            enhanced_parts.append(enhanced_part)

        # ç¼“å­˜ç»“æœ
        _video_parts_cache[cache_key] = enhanced_parts
        return enhanced_parts

    except (json.JSONDecodeError, Exception) as e:
        print(f"å¼‚æ­¥è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        return None

def get_video_parts_with_covers(bvid: str):
    """Fetches video parts and their cover images from Bilibili page."""
    try:
        # è·å–è§†é¢‘é¡µé¢
        url = f"https://www.bilibili.com/video/{bvid}"
        response = get_bilibili_response(url)
        if not response:
            return None

        html_content = response.text

        # æå–JSONæ•°æ®
        match = re.search(r'<script>window\.__INITIAL_STATE__=(.*?);\(function\(\)', html_content)
        if not match:
            print(f"æœªæ‰¾åˆ°è§†é¢‘æ•°æ®: {bvid}")
            return None

        json_data_string = match.group(1)
        data = json.loads(json_data_string)

        # è·å–è§†é¢‘åˆ†Påˆ—è¡¨
        video_parts = data.get('videoData', {}).get('pages', [])
        if not video_parts:
            print(f"æœªæ‰¾åˆ°åˆ†Pè§†é¢‘: {bvid}")
            return None

        # ä¸ºæ¯ä¸ªåˆ†Pæ·»åŠ å°é¢ä¿¡æ¯
        enhanced_parts = []
        for part in video_parts:
            enhanced_part = {
                'cid': part.get('cid'),
                'page': part.get('page'),
                'part': part.get('part'),
                'duration': part.get('duration'),
                'cover_url': part.get('first_frame', ''),
                'dimension': part.get('dimension', {})
            }
            enhanced_parts.append(enhanced_part)

        return enhanced_parts

    except (json.JSONDecodeError, Exception) as e:
        print(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        return None

async def get_video_parts_async(bvid: str) -> Optional[List[Dict]]:
    """å¼‚æ­¥è·å–è§†é¢‘åˆ†PåŸºæœ¬ä¿¡æ¯ï¼Œå¸¦ç¼“å­˜ä¸å¤–å‘¼é™æµ"""
    # æ£€æŸ¥ç¼“å­˜
    cache_key = f"parts_{bvid}"
    if cache_key in _video_parts_cache:
        return _video_parts_cache[cache_key]

    try:
        url = 'https://api.bilibili.com/x/player/pagelist'
        params = {'bvid': bvid, 'jsonp': 'jsonp'}

        response = await limited_get(url, params=params)
        if response and response.status == 200:
            data = await response.json()
            if data['code'] == 0:
                result = data['data']
                # ç¼“å­˜ç»“æœ
                _video_parts_cache[cache_key] = result
                return result
    except Exception as e:
        print(f"å¼‚æ­¥è·å–è§†é¢‘åˆ†På¤±è´¥: {e}")
    return None

def get_video_parts(bvid: str):
    """Fetches the list of video parts (pages) for a given Bilibili BV ID."""
    url = 'https://api.bilibili.com/x/player/pagelist'
    params = {'bvid': bvid, 'jsonp': 'jsonp'}
    response = get_bilibili_response(url, params)
    if response:
        try:
            data = response.json()
            if data['code'] == 0:
                return data['data']
        except (ValueError, KeyError):
            return None
    return None

async def download_and_cache_cover_async(bvid: str, page: int, cover_url: str) -> str:
    """å¼‚æ­¥ä¸‹è½½å¹¶ç¼“å­˜å°é¢å›¾ç‰‡ï¼Œè¿”å›æœ¬åœ°è·¯å¾„ï¼ˆå—é™æµç®¡æ§ï¼‰"""
    if not cover_url:
        return ""

    # ç¡®ä¿URLåè®®å®Œæ•´
    if cover_url.startswith('//'):
        cover_url = 'http:' + cover_url

    # ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
    cover_filename = f"{bvid}_p{page}.jpg"
    cover_path = COVERS_DIR / cover_filename

    # å¦‚æœå·²ç»ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if cover_path.exists():
        return f"/covers/{cover_filename}"

    try:
        response = await limited_get(cover_url)
        if response and response.status == 200:
            content = await response.read()
            with open(cover_path, 'wb') as f:
                f.write(content)
            return f"/covers/{cover_filename}"
    except Exception as e:
        print(f"å¼‚æ­¥ä¸‹è½½å°é¢å¤±è´¥: {e}")

    return ""


async def check_subtitle_availability_async(bvid: str, page: int, cid: int) -> bool:
    """å¼‚æ­¥æ£€æŸ¥è§†é¢‘æ˜¯å¦æœ‰å­—å¹•å¯ç”¨"""
    try:
        # å¦‚æœæ²¡æœ‰é…ç½®Cookieï¼Œç›´æ¥è¿”å›False
        if not BILIBILI_COOKIE:
            return False

        # è·å–WBIç­¾åå¯†é’¥
        wbi_key = await get_wbi_keys_async(BILIBILI_COOKIE)
        if not wbi_key:
            return False

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {'bvid': bvid, 'cid': cid}
        signed_params = sign_wbi_params(params, wbi_key)

        # å¼‚æ­¥è¯·æ±‚å­—å¹•API
        session = await get_http_session()
        player_api_url = "https://api.bilibili.com/x/player/wbi/v2"
        headers = {'Cookie': BILIBILI_COOKIE}

        async with session.get(player_api_url, params=signed_params, headers=headers) as response:
            if response.status != 200:
                return False

            subtitle_data = await response.json()
            if subtitle_data.get('code') != 0:
                return False

            subtitles_list = subtitle_data.get('data', {}).get('subtitle', {}).get('subtitles', [])
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ä¸Šä¼ çš„å­—å¹•
            user_subtitle = next((s for s in subtitles_list if s.get('ai_type') == 0 and s.get('subtitle_url')), None)
            return user_subtitle is not None

    except Exception as e:
        print(f"å¼‚æ­¥æ£€æŸ¥å­—å¹•å¯ç”¨æ€§å¤±è´¥: {e}")
        return False

async def check_subtitle_availability(bvid: str, page: int, cid: int) -> bool:
    """æ£€æŸ¥è§†é¢‘æ˜¯å¦æœ‰å­—å¹•å¯ç”¨"""
    try:
        print(f"æ£€æŸ¥å­—å¹•å¯ç”¨æ€§: bvid={bvid}, page={page}, cid={cid}")

        # å¦‚æœæ²¡æœ‰é…ç½®Cookieï¼Œç›´æ¥è¿”å›False
        if not BILIBILI_COOKIE:
            print("æœªé…ç½®Bç«™Cookieï¼Œå­—å¹•åŠŸèƒ½ä¸å¯ç”¨")
            return False

        # è·å–WBIç­¾åå¯†é’¥
        wbi_key = get_wbi_keys(BILIBILI_COOKIE)
        if not wbi_key:
            print("è·å–WBIå¯†é’¥å¤±è´¥")
            return False

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {'bvid': bvid, 'cid': cid}
        signed_params = sign_wbi_params(params, wbi_key)

        print(f"è¯·æ±‚å‚æ•°: {signed_params}")

        # è¯·æ±‚å­—å¹•APIï¼Œå¸¦ä¸ŠCookie
        player_api_url = "https://api.bilibili.com/x/player/wbi/v2"
        headers = HEADERS.copy()
        headers['Cookie'] = BILIBILI_COOKIE

        response = limited_get_sync(player_api_url, params=signed_params, headers=headers)

        if not response:
            print("å­—å¹•APIè¯·æ±‚å¤±è´¥")
            return False

        subtitle_data = response.json()
        print(f"å­—å¹•APIå“åº”: code={subtitle_data.get('code')}, message={subtitle_data.get('message')}")

        if subtitle_data.get('code') != 0:
            print(f"å­—å¹•APIè¿”å›é”™è¯¯: {subtitle_data.get('message')}")
            return False

        subtitles_list = subtitle_data.get('data', {}).get('subtitle', {}).get('subtitles', [])
        print(f"æ‰¾åˆ°å­—å¹•åˆ—è¡¨: {len(subtitles_list)} ä¸ª")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ä¸Šä¼ çš„å­—å¹•
        user_subtitle = next((s for s in subtitles_list if s.get('ai_type') == 0 and s.get('subtitle_url')), None)

        has_subtitle = user_subtitle is not None
        print(f"ç”¨æˆ·å­—å¹•å¯ç”¨: {has_subtitle}")

        return has_subtitle

    except Exception as e:
        print(f"æ£€æŸ¥å­—å¹•å¯ç”¨æ€§å¤±è´¥: {e}")
        return False



async def download_and_cache_subtitle(bvid: str, page: int, cid: int) -> str:
    """ä¸‹è½½å¹¶ç¼“å­˜å­—å¹•æ–‡ä»¶ï¼Œè¿”å›æœ¬åœ°è·¯å¾„"""
    try:
        print(f"å¼€å§‹ä¸‹è½½å­—å¹•: bvid={bvid}, page={page}, cid={cid}")

        # ç”Ÿæˆå­—å¹•æ–‡ä»¶å
        subtitle_filename = f"{bvid}_p{page}.vtt"
        subtitle_path = SUBTITLES_DIR / subtitle_filename

        # å¦‚æœå·²ç»ç¼“å­˜ï¼Œç›´æ¥è¿”å›
        if subtitle_path.exists():
            print(f"å­—å¹•å·²ç¼“å­˜: {subtitle_filename}")
            return f"/subtitles/{subtitle_filename}"

        # å¦‚æœæ²¡æœ‰é…ç½®Cookieï¼Œç›´æ¥è¿”å›ç©º
        if not BILIBILI_COOKIE:
            print("æœªé…ç½®Bç«™Cookieï¼Œæ— æ³•ä¸‹è½½å­—å¹•")
            return ""

        # è·å–WBIç­¾åå¯†é’¥
        wbi_key = get_wbi_keys(BILIBILI_COOKIE)
        if not wbi_key:
            return ""

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {'bvid': bvid, 'cid': cid}
        signed_params = sign_wbi_params(params, wbi_key)

        # è¯·æ±‚å­—å¹•APIï¼Œå¸¦ä¸ŠCookie
        player_api_url = "https://api.bilibili.com/x/player/wbi/v2"
        headers = HEADERS.copy()
        headers['Cookie'] = BILIBILI_COOKIE

        response = requests.get(player_api_url, params=signed_params, headers=headers)

        if not response or response.status_code != 200:
            print("å­—å¹•APIè¯·æ±‚å¤±è´¥")
            return ""

        subtitle_data = response.json()
        if subtitle_data.get('code') != 0:
            print(f"å­—å¹•APIè¿”å›é”™è¯¯: {subtitle_data.get('code')} - {subtitle_data.get('message')}")
            return ""

        subtitles_list = subtitle_data.get('data', {}).get('subtitle', {}).get('subtitles', [])
        # æŸ¥æ‰¾ç”¨æˆ·ä¸Šä¼ çš„å­—å¹•
        user_subtitle = next((s for s in subtitles_list if s.get('ai_type') == 0 and s.get('subtitle_url')), None)

        if not user_subtitle:
            return ""

        # ä¸‹è½½å­—å¹•å†…å®¹
        subtitle_url = user_subtitle.get('subtitle_url')
        if subtitle_url.startswith('//'):
            subtitle_url = 'https:' + subtitle_url

        subtitle_response = limited_get_sync(subtitle_url, headers=HEADERS)
        if not subtitle_response:
            return ""

        subtitle_content = subtitle_response.json()

        # è½¬æ¢ä¸ºWebVTTæ ¼å¼å¹¶ä¿å­˜
        with open(subtitle_path, 'w', encoding='utf-8') as f:
            f.write("WEBVTT\n\n")
            for line in subtitle_content.get('body', []):
                start_time = format_webvtt_time(line.get('from', 0))
                end_time = format_webvtt_time(line.get('to', 0))
                content = line.get('content', '')
                f.write(f"{start_time} --> {end_time}\n{content}\n\n")

        return f"/subtitles/{subtitle_filename}"

    except Exception as e:
        print(f"ä¸‹è½½å­—å¹•å¤±è´¥: {e}")
        return ""

def format_webvtt_time(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸ºWebVTTæ—¶é—´æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{secs:06.3f}"

def download_and_merge(bvid: str, p_info: dict, target_dir: Path):
    """Downloads and merges a single video part."""
    page = p_info['page']
    cid = p_info['cid']
    # Sanitize the title to create a valid filename
    clean_name = re.sub(r'[\\/*?:"<>|]', "", p_info['part'])
    final_video_path = target_dir / f"{clean_name}.mp4"
    
    # If the final merged video already exists, do nothing.
    if final_video_path.exists():
        print(f"Video '{clean_name}.mp4' already exists. Skipping download.")
        return str(final_video_path)

    # 1. Get Session
    session_url = f'https://www.bilibili.com/video/{bvid}?p={page}'
    session_response = get_bilibili_response(session_url)
    if not session_response:
        raise Exception("Failed to get session.")
    
    session_match = re.search(r'"session":"(.*?)"', session_response.text)
    if not session_match:
        raise Exception("Could not find session in page.")
    session = session_match.group(1)

    # 2. Get Video/Audio URLs
    playurl = 'https://api.bilibili.com/x/player/playurl'
    params = {
        'cid': cid, 'bvid': bvid, 'qn': '80', # qn=80 for 1080p
        'fnver': '0', 'fnval': '976', 'session': session
    }
    play_response = get_bilibili_response(playurl, params)
    if not play_response:
        raise Exception("Failed to get play URLs.")
        
    play_data = play_response.json()
    if play_data['code'] != 0:
        raise Exception(f"API error getting play URLs: {play_data.get('message', 'Unknown error')}")

    try:
        audio_url = play_data['data']['dash']['audio'][0]['baseUrl']
        video_url = play_data['data']['dash']['video'][0]['baseUrl']
    except (KeyError, IndexError):
        raise Exception("Could not parse audio/video URLs from API response.")

    # 3. Download Audio and Video
    temp_audio_path = target_dir / f"{clean_name}_audio.mp3"
    temp_video_path = target_dir / f"{clean_name}_video.mp4"

    audio_res = get_bilibili_response(audio_url)
    video_res = get_bilibili_response(video_url)

    if not audio_res or not video_res:
        raise Exception("Failed to download audio or video content.")

    with open(temp_audio_path, 'wb') as f:
        f.write(audio_res.content)
    with open(temp_video_path, 'wb') as f:
        f.write(video_res.content)

    # 4. Merge with ffmpeg
    command = [
        'ffmpeg',
        '-i', str(temp_video_path),
        '-i', str(temp_audio_path),
        '-c', 'copy',
        str(final_video_path)
    ]
    try:
        subprocess.run(command, shell=False, check=True, capture_output=True, text=True)
    except subprocess.CalledProcessError as e:
        # If merge fails, clean up temp files and raise error
        temp_audio_path.unlink(missing_ok=True)
        temp_video_path.unlink(missing_ok=True)
        raise Exception(f"ffmpeg merge failed: {e.stderr}")

    # 5. Clean up temporary files
    temp_audio_path.unlink(missing_ok=True)
    temp_video_path.unlink(missing_ok=True)
    
    return str(final_video_path)


# --- API Endpoints ---

def scan_folders_recursive(base_path: Path, current_path: Path = None, depth: int = 0, max_depth: int = 10) -> List[dict]:
    """é€’å½’æ‰«ææ–‡ä»¶å¤¹ç»“æ„"""
    if current_path is None:
        current_path = base_path
    
    if depth > max_depth:
        return []
    
    folders = []
    
    try:
        for item in current_path.iterdir():
            if item.is_dir():
                # è®¡ç®—ç›¸å¯¹è·¯å¾„
                relative_path = str(item.relative_to(base_path))
                parent_path = str(current_path.relative_to(base_path)) if current_path != base_path else ""
                
                # æ£€æŸ¥æ˜¯å¦æœ‰list.txtæ–‡ä»¶
                list_file = item / "list.txt"
                has_list_file = list_file.exists()
                
                # é€’å½’æ‰«æå­æ–‡ä»¶å¤¹
                children = scan_folders_recursive(base_path, item, depth + 1, max_depth)
                
                folder_info = {
                    "name": item.name,
                    "path": relative_path,
                    "parent_path": parent_path if parent_path else None,
                    "children": children,
                    "has_list_file": has_list_file,
                    "video_count": 0,
                    "downloaded_count": 0,
                    "depth": depth,
                    "is_folder": True
                }
                
                folders.append(folder_info)
    except PermissionError:
        pass
    
    # å¯¹æ–‡ä»¶å¤¹è¿›è¡Œä¸­æ–‡å‹å¥½æ’åº
    return sort_folders_chinese(folders)

@app.get("/api/folders")
async def list_folders(path: str = ""):
    """è·å–æ–‡ä»¶å¤¹åˆ—è¡¨ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„"""
    if not VIDEOS_DIR.is_dir():
        return JSONResponse(content=[], headers={"Content-Type": "application/json; charset=utf-8"})
    
    # ç¡®å®šè¦æ‰«æçš„ç›®å½•
    if path and path.strip():
        target_path = VIDEOS_DIR / path
        if not target_path.exists() or not target_path.is_dir():
            raise HTTPException(status_code=404, detail=f"Folder not found: {path}")
    else:
        target_path = VIDEOS_DIR
    
    # ç›´æ¥æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„ç›´æ¥å­æ–‡ä»¶å¤¹
    folders = []
    try:
        for item in target_path.iterdir():
            if item.is_dir():
                relative_path = str(item.relative_to(VIDEOS_DIR)).replace('\\', '/')  # ç¡®ä¿ä½¿ç”¨æ­£æ–œæ 
                list_file = item / "list.txt"
                has_list_file = list_file.exists()
                
                folder_info = {
                    "name": item.name,
                    "path": relative_path,
                    "parent_path": path if path and path.strip() else None,
                    "children": [],
                    "has_list_file": has_list_file,
                    "video_count": 0,
                    "downloaded_count": 0,
                    "depth": len(path.split('/')) if path and path.strip() else 0,
                    "is_folder": True
                }
                folders.append(folder_info)
    except PermissionError:
        pass
    
    # å¯¹æ–‡ä»¶å¤¹è¿›è¡Œä¸­æ–‡å‹å¥½æ’åº
    sorted_folders = sort_folders_chinese(folders)
    return JSONResponse(content=sorted_folders, headers={"Content-Type": "application/json; charset=utf-8"})

@app.get("/api/folders/{folder_path:path}")
async def list_videos_in_folder(folder_path: str):
    """
    å¿«é€Ÿè¿”å›è§†é¢‘åˆ—è¡¨åŸºæœ¬ä¿¡æ¯ï¼Œå®ç°åˆ†é˜¶æ®µåŠ è½½
    ç¬¬ä¸€é˜¶æ®µï¼šç«‹å³è¿”å›åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€åˆ†Pæ•°é‡ï¼‰
    """
    target_folder = VIDEOS_DIR / folder_path
    list_file = target_folder / "list.txt"

    if not list_file.exists():
        raise HTTPException(status_code=404, detail=f"'list.txt' not found in folder '{folder_path}'")

    with open(list_file, 'r', encoding='utf-8') as f:
        bvid_lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    if not bvid_lines:
        raise HTTPException(status_code=404, detail=f"'list.txt' is empty or contains no valid BV IDs.")

    try:
        bvid = extract_bvid_from_url(bvid_lines[0])
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # ä½¿ç”¨å¼‚æ­¥å‡½æ•°è·å–åŸºæœ¬ä¿¡æ¯ï¼Œä¼˜å…ˆå°è¯•è¯¦ç»†ä¿¡æ¯
    video_parts = await get_video_parts_async(bvid)
    if not video_parts:
        raise HTTPException(status_code=500, detail=f"Could not fetch video parts for BV ID: {bvid}")

    # å¿«é€Ÿè¿”å›åŸºæœ¬ä¿¡æ¯ï¼Œä¸åŒ…å«å°é¢å’Œè¯¦ç»†ä¿¡æ¯
    enhanced_parts = []
    for part in video_parts:
        enhanced_parts.append({
            "title": part['part'],
            "page": part['page'],
            "cover_url": "",  # ç¨åå¼‚æ­¥åŠ è½½
            "duration": part.get('duration', 0),
            "cid": part['cid'],
            "bvid": bvid,
            "has_subtitle": None  # ç¨åå¼‚æ­¥æ£€æŸ¥
        })

    return JSONResponse(content=enhanced_parts, headers={"Content-Type": "application/json; charset=utf-8"})

@app.get("/api/folders/{folder_path:path}/details")
async def get_videos_details(folder_path: str):
    """
    ç¬¬äºŒé˜¶æ®µï¼šè·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯ï¼ˆå°é¢ã€å­—å¹•çŠ¶æ€ç­‰ï¼‰
    """
    target_folder = VIDEOS_DIR / folder_path
    list_file = target_folder / "list.txt"

    if not list_file.exists():
        raise HTTPException(status_code=404, detail=f"'list.txt' not found in folder '{folder_path}'")

    with open(list_file, 'r', encoding='utf-8') as f:
        bvid_lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    if not bvid_lines:
        raise HTTPException(status_code=404, detail=f"'list.txt' is empty or contains no valid BV IDs.")

    try:
        bvid = extract_bvid_from_url(bvid_lines[0])
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«å°é¢URLï¼‰
    video_parts = await get_video_parts_with_covers_async(bvid)
    if not video_parts:
        raise HTTPException(status_code=500, detail=f"Could not fetch detailed video parts for BV ID: {bvid}")

    # è¿”å›è¯¦ç»†ä¿¡æ¯
    detailed_parts = []
    for part in video_parts:
        # å¼‚æ­¥æ£€æŸ¥å­—å¹•å¯ç”¨æ€§
        has_subtitle = await check_subtitle_availability_async(bvid, part['page'], part['cid'])

        detailed_parts.append({
            "page": part['page'],
            "cover_source": part.get('cover_url', ''),
            "duration": part.get('duration', 0),
            "has_subtitle": has_subtitle
        })

    return JSONResponse(content=detailed_parts, headers={"Content-Type": "application/json; charset=utf-8"})

@app.get("/api/batch/covers/{bvid}")
async def get_batch_covers(bvid: str, pages: str):
    """
    æ‰¹é‡è·å–å¤šä¸ªåˆ†Pçš„å°é¢
    pages: é€—å·åˆ†éš”çš„é¡µç ï¼Œå¦‚ "1,2,3"
    """
    try:
        page_numbers = [int(p.strip()) for p in pages.split(',') if p.strip().isdigit()]
        if not page_numbers:
            return {"covers": {}}

        # è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
        video_parts = await get_video_parts_with_covers_async(bvid)
        if not video_parts:
            return {"covers": {}}

        # åˆ›å»ºé¡µç åˆ°å°é¢URLçš„æ˜ å°„
        page_to_cover = {}
        for part in video_parts:
            if part['page'] in page_numbers:
                page_to_cover[part['page']] = part.get('cover_url', '')

        # æ‰¹é‡ä¸‹è½½å°é¢
        covers = {}
        for page_num in page_numbers:
            cover_url = page_to_cover.get(page_num, '')
            if cover_url:
                # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
                cover_filename = f"{bvid}_p{page_num}.jpg"
                cover_path = COVERS_DIR / cover_filename

                if cover_path.exists():
                    covers[str(page_num)] = f"/covers/{cover_filename}"
                else:
                    # å¼‚æ­¥ä¸‹è½½
                    downloaded_url = await download_and_cache_cover_async(bvid, page_num, cover_url)
                    if downloaded_url:
                        covers[str(page_num)] = downloaded_url

        return JSONResponse(content={"covers": covers}, headers={"Content-Type": "application/json; charset=utf-8"})

    except Exception as e:
        print(f"æ‰¹é‡è·å–å°é¢å¤±è´¥: {e}")
        return JSONResponse(content={"covers": {}}, headers={"Content-Type": "application/json; charset=utf-8"})



@app.get("/api/cover/{bvid}/{page_number}")
async def get_video_cover(bvid: str, page_number: int):
    """å¼‚æ­¥è·å–å•ä¸ªè§†é¢‘çš„å°é¢ï¼Œä¼˜åŒ–æ€§èƒ½"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç»ç¼“å­˜
        cover_filename = f"{bvid}_p{page_number}.jpg"
        cover_path = COVERS_DIR / cover_filename

        if cover_path.exists():
            return JSONResponse(content={"cover_url": f"/covers/{cover_filename}", "cached": True}, headers={"Content-Type": "application/json; charset=utf-8"})

        # ä½¿ç”¨å¼‚æ­¥å‡½æ•°è·å–è§†é¢‘ä¿¡æ¯
        video_parts = await get_video_parts_with_covers_async(bvid)
        if not video_parts:
            return JSONResponse(content={"cover_url": "", "cached": False}, headers={"Content-Type": "application/json; charset=utf-8"})

        # æ‰¾åˆ°å¯¹åº”çš„åˆ†P
        target_part = None
        for part in video_parts:
            if part['page'] == page_number:
                target_part = part
                break

        if not target_part or not target_part.get('cover_url'):
            return JSONResponse(content={"cover_url": "", "cached": False}, headers={"Content-Type": "application/json; charset=utf-8"})

        # ä¸‹è½½å¹¶ç¼“å­˜å°é¢
        cover_url = await download_and_cache_cover_async(bvid, page_number, target_part['cover_url'])
        return JSONResponse(content={"cover_url": cover_url, "cached": False}, headers={"Content-Type": "application/json; charset=utf-8"})

    except Exception as e:
        print(f"è·å–å°é¢å¤±è´¥: {e}")
        return JSONResponse(content={"cover_url": "", "cached": False}, headers={"Content-Type": "application/json; charset=utf-8"})

@app.post("/api/covers/preload")
async def preload_covers(request_data: dict):
    """
    é¢„åŠ è½½å°é¢ï¼Œç”¨äºæå‡ç”¨æˆ·ä½“éªŒ
    request_data: {"bvid": "BV1xx", "pages": [1, 2, 3]}
    """
    try:
        bvid = request_data.get('bvid')
        pages = request_data.get('pages', [])

        if not bvid or not pages:
            return {"status": "error", "message": "Missing bvid or pages"}

        # è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
        video_parts = await get_video_parts_with_covers_async(bvid)
        if not video_parts:
            return {"status": "error", "message": "Could not fetch video parts"}

        # åˆ›å»ºé¡µç åˆ°å°é¢URLçš„æ˜ å°„
        page_to_cover = {}
        for part in video_parts:
            if part['page'] in pages:
                page_to_cover[part['page']] = part.get('cover_url', '')

        # å¼‚æ­¥é¢„åŠ è½½å°é¢ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
        preload_tasks = []
        for page_num in pages:
            cover_url = page_to_cover.get(page_num, '')
            if cover_url:
                # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
                cover_filename = f"{bvid}_p{page_num}.jpg"
                cover_path = COVERS_DIR / cover_filename

                if not cover_path.exists():
                    # åˆ›å»ºé¢„åŠ è½½ä»»åŠ¡
                    task = asyncio.create_task(
                        download_and_cache_cover_async(bvid, page_num, cover_url)
                    )
                    preload_tasks.append(task)

        # å¯åŠ¨é¢„åŠ è½½ä»»åŠ¡ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
        if preload_tasks:
            asyncio.create_task(asyncio.gather(*preload_tasks, return_exceptions=True))

        return {"status": "success", "preloading": len(preload_tasks)}

    except Exception as e:
        print(f"é¢„åŠ è½½å°é¢å¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}


@app.get("/api/play/{folder_path:path}/{page_number}")
async def play_video(folder_path: str, page_number: int):
    """
    æ’­æ”¾è§†é¢‘ï¼ŒåŒ…å«å­—å¹•æ£€æŸ¥ï¼ˆæ¢å¤åŸæœ‰åŠŸèƒ½ï¼‰
    """
    target_folder = VIDEOS_DIR / folder_path
    list_file = target_folder / "list.txt"

    if not list_file.exists():
        raise HTTPException(status_code=404, detail=f"'list.txt' not found in folder '{folder_path}'")

    with open(list_file, 'r', encoding='utf-8') as f:
        bvid_lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    if not bvid_lines:
        raise HTTPException(status_code=404, detail="'list.txt' is empty.")

    try:
        bvid = extract_bvid_from_url(bvid_lines[0])
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # ä½¿ç”¨å¼‚æ­¥å‡½æ•°è·å–è§†é¢‘åˆ†Pä¿¡æ¯
    video_parts = await get_video_parts_async(bvid)
    if not video_parts:
        raise HTTPException(status_code=500, detail="Could not fetch video parts.")

    target_part = None
    for part in video_parts:
        if part['page'] == page_number:
            target_part = part
            break

    if not target_part:
        raise HTTPException(status_code=404, detail=f"Page number {page_number} not found for this BV ID.")

    clean_name = re.sub(r'[\\/*?:"<>|]', "", target_part['part'])
    final_video_path = target_folder / f"{clean_name}.mp4"

    # æ£€æŸ¥å­—å¹•å¯ç”¨æ€§å’Œè·å–å­—å¹•ï¼ˆæ¢å¤åŸæœ‰åŠŸèƒ½ï¼‰
    has_subtitle = await check_subtitle_availability(bvid, page_number, target_part['cid'])
    subtitle_url = ""
    if has_subtitle:
        subtitle_url = await download_and_cache_subtitle(bvid, page_number, target_part['cid'])

    # If file exists, return its path immediately.
    if final_video_path.exists():
        return {
            "status": "ready",
            "video_url": f"/static/{folder_path}/{final_video_path.name}",
            "has_subtitle": has_subtitle,
            "subtitle_url": subtitle_url
        }

    # If file does not exist, start download and return a "pending" status.
    try:
        # ä½¿ç”¨å¼‚æ­¥çº¿ç¨‹æ± ä¸‹è½½
        await asyncio.to_thread(download_and_merge, bvid, target_part, target_folder)
        return {
            "status": "ready",
            "video_url": f"/static/{folder_path}/{final_video_path.name}",
            "has_subtitle": has_subtitle,
            "subtitle_url": subtitle_url
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to download video: {str(e)}")




@app.get("/static/{folder_path:path}/{file_name}")
async def serve_static_video(folder_path: str, file_name: str):
    """Serves the video files statically."""
    file_path = VIDEOS_DIR / folder_path / file_name
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="File not found.")
    return FileResponse(file_path)

@app.get("/covers/{file_name}")
async def serve_cover_image(file_name: str):
    """Serves the cover images statically."""
    file_path = COVERS_DIR / file_name
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Cover image not found.")
    return FileResponse(file_path)

@app.get("/subtitles/{file_name}")
async def serve_subtitle_file(file_name: str):
    """Serves the subtitle files statically."""
    file_path = SUBTITLES_DIR / file_name
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Subtitle file not found.")
    return FileResponse(file_path, media_type="text/vtt")

@app.get("/api/subtitle/{folder_path:path}/{page_number}")
async def get_subtitle(folder_path: str, page_number: int):
    """è·å–æŒ‡å®šè§†é¢‘çš„å­—å¹•æ–‡ä»¶"""
    target_folder = VIDEOS_DIR / folder_path
    list_file = target_folder / "list.txt"

    if not list_file.exists():
        raise HTTPException(status_code=404, detail=f"'list.txt' not found in folder '{folder_path}'")

    with open(list_file, 'r', encoding='utf-8') as f:
        bvid_lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    if not bvid_lines:
        raise HTTPException(status_code=404, detail="'list.txt' is empty.")

    try:
        bvid = extract_bvid_from_url(bvid_lines[0])
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # è·å–è§†é¢‘åˆ†Pä¿¡æ¯
    video_parts = get_video_parts_with_covers(bvid)
    if not video_parts:
        video_parts = get_video_parts(bvid)
        if not video_parts:
            raise HTTPException(status_code=500, detail="Could not fetch video parts.")

    # æ‰¾åˆ°å¯¹åº”çš„åˆ†P
    target_part = None
    for part in video_parts:
        if part['page'] == page_number:
            target_part = part
            break

    if not target_part:
        raise HTTPException(status_code=404, detail=f"Page number {page_number} not found.")

    # ä¸‹è½½å¹¶ç¼“å­˜å­—å¹•
    subtitle_path = await download_and_cache_subtitle(bvid, page_number, target_part['cid'])

    if not subtitle_path:
        raise HTTPException(status_code=404, detail="No subtitle available for this video.")

    return {"subtitle_url": subtitle_path}

# --- Frontend Routes ---
@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    """æœåŠ¡å‰ç«¯ä¸»é¡µ"""
    index_file = FRONTEND_DIR / "index.html"
    if index_file.exists():
        return HTMLResponse(content=index_file.read_text(encoding='utf-8'))
    return HTMLResponse("<h1>Frontend not found</h1>", status_code=404)

@app.get("/{file_path:path}")
async def serve_frontend_files(file_path: str):
    """æœåŠ¡å‰ç«¯é™æ€æ–‡ä»¶"""
    file = FRONTEND_DIR / file_path
    if file.exists() and file.is_file():
        # ä¸ºå‰ç«¯æ–‡ä»¶æ·»åŠ ç¼“å­˜æ§åˆ¶å¤´
        if file_path.endswith(('.js', '.css')):
            headers = {
                "Cache-Control": "no-cache, no-store, must-revalidate",
                "Pragma": "no-cache",
                "Expires": "0"
            }
            return FileResponse(file, headers=headers)
        return FileResponse(file)
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ä¸»é¡µï¼ˆç”¨äºSPAè·¯ç”±ï¼‰
    index_file = FRONTEND_DIR / "index.html"
    if index_file.exists():
        return HTMLResponse(content=index_file.read_text(encoding='utf-8'))
    return HTMLResponse("<h1>File not found</h1>", status_code=404)

# --- åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ---
@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†èµ„æº"""
    await close_http_session()
    print("ğŸ”„ HTTPä¼šè¯å·²å…³é—­")

# --- Main Execution ---
if __name__ == "__main__":
    import uvicorn
    print("ğŸ¬ å„¿ç«¥è§†é¢‘æ’­æ”¾å™¨æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print(f"ğŸ“ è§†é¢‘ç›®å½•: {VIDEOS_DIR.resolve()}")
    print(f"ğŸŒ å‰ç«¯ç›®å½•: {FRONTEND_DIR.resolve()}")
    print("ğŸš€ æœåŠ¡åœ°å€: http://localhost:8000")
    print("âš¡ æ€§èƒ½ä¼˜åŒ–ï¼šå¼‚æ­¥ç½‘ç»œè¯·æ±‚ + åˆ†é˜¶æ®µåŠ è½½")
    uvicorn.run(app, host="0.0.0.0", port=8000)